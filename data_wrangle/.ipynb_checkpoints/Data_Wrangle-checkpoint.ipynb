{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "import io\n",
    "import urllib\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from collections import defaultdict\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aregel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text cleaning imports\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and Clean Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using http://www.trumptwitterarchive.com/archive , I downloaded all of Donald Trump's tweets from 01/01/2016 - 10/11/2017 9:19 am MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath to twitter archive data.\n",
    "twitter_json = r'C:\\Users\\aregel\\Documents\\springboard\\Twitter-Politics\\data_wrangle\\data\\twitter_01_01_16_to_10-30-17.json' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas I will read the twitter json file, convert it to a dataframe, set the index to 'created at' as datetime objects, then write it to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2017-10-29 14:48:09', '2017-10-29 14:17:14',\n",
       "               '2017-10-29 14:09:51', '2017-10-29 14:02:37',\n",
       "               '2017-10-29 13:53:43', '2017-10-29 13:12:47',\n",
       "               '2017-10-28 23:15:39', '2017-10-28 22:19:42',\n",
       "               '2017-10-28 22:08:30', '2017-10-28 21:10:36',\n",
       "               ...\n",
       "               '2016-01-02 03:26:54', '2016-01-01 23:40:51',\n",
       "               '2016-01-01 23:25:54', '2016-01-01 23:24:13',\n",
       "               '2016-01-01 23:10:25', '2016-01-01 23:08:18',\n",
       "               '2016-01-01 23:06:09', '2016-01-01 23:02:05',\n",
       "               '2016-01-01 23:00:09', '2016-01-01 21:29:56'],\n",
       "              dtype='datetime64[ns]', name='created_at', length=6343, freq=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the json data into a pandas dataframe\n",
    "tweet_data = pd.read_json(twitter_json)\n",
    "# set column 'created_at' to the index\n",
    "tweet_data.set_index('created_at', drop=True, inplace= True)\n",
    "# convert timestamp index to a datetime index\n",
    "pd.to_datetime(tweet_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to add columns with tokenized text and identify twitter specific puncutiations like hashtags and @ mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify hash tags\n",
    "def hash_tag(text):\n",
    "    return re.findall(r'(#[^\\s]+)', text) \n",
    "# function to identify @mentions\n",
    "def at_tag(text):\n",
    "    return re.findall(r'(@[A-Za-z_]+)[^s]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all the tweet's text\n",
    "tweet_data['text_tokenized'] = tweet_data['text'].apply(lambda x: word_tokenize(x))\n",
    "# apply hash tag function to text column\n",
    "tweet_data['hash_tags'] = tweet_data['text'].apply(lambda x: hash_tag(x))\n",
    "# apply at_tag function to text column\n",
    "tweet_data['@_tags'] = tweet_data['text'].apply(lambda x: at_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle tweet data so that loading and cleaning only needs to be done once\n",
    "tweet_pickle_path = r'C:\\Users\\aregel\\Documents\\springboard\\Twitter-Politics\\data_wrangle\\data\\twitter_01_01_16_to_10-30-17.pickle'\n",
    "tweet_data.to_pickle(tweet_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scrape Data from the Federal Register\n",
    "## *This has already been done, and all of the pdfs published by the Executive Office of the U.S.A are in the data folder from 2016/01/01 - 2017/10/15*\n",
    "\n",
    "## *Don't execute this code unless you need more up-to-date information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 2016, 2017 url that contains all of the Executive Office of the President's published documents\n",
    "executive_office_url_2016 = r'https://www.federalregister.gov/index/2016/executive-office-of-the-president'\n",
    "executive_office_url_2017 = r'https://www.federalregister.gov/index/2017/executive-office-of-the-president' \n",
    "\n",
    "# scrape all urls for pdf documents published 2016-01-01 to 2017-10-30 by the U.S.A. Executive Office\n",
    "pdf_urls= []\n",
    "for url in [executive_office_url_2016, executive_office_url_2017]:\n",
    "    response = requests.get(url)\n",
    "    pattern = re.compile(r'https:.*\\.pdf')\n",
    "    pdfs = re.findall(pattern, response.text)\n",
    "    pdf_urls.append(pdfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes all of the pdfs to the data folder\n",
    "start = 'data/'\n",
    "end = '.pdf'\n",
    "num = 0\n",
    "for i in range(0,(len(pdf_urls))):\n",
    "    for url in pdf_urls[i]:\n",
    "        ver = str(num)\n",
    "        pdf_path = start + ver + end\n",
    "        r = requests.get(url)\n",
    "        file = open(pdf_path, 'wb')\n",
    "        file.write(r.content)\n",
    "        file.close()\n",
    "        num = num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe with the date the pdf was published and the text of each pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert pdf to text from stack overflow (https://stackoverflow.com/questions/26494211/extracting-text-from-a-pdf-file-using-pdfminer-in-python/44476759#44476759)\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "# finds the first time the name of a day appears in the txt, and returns that name\n",
    "def find_day(word_generator):\n",
    "    day_list = ['Monday,', 'Tuesday,', 'Wednesday,', 'Thursday,', 'Friday,', 'Saturday,', 'Sunday,']\n",
    "    day_name_dict = {'Mon':'Monday,', 'Tue':'Tuesday,','Wed':'Wednesday,','Thu':'Thursday,','Fri':'Friday,','Sat':'Saturday,','Sun':'Sunday,'}\n",
    "    day_name = []\n",
    "    for val in word_generator:\n",
    "        if val in day_list:\n",
    "            num_position = txt.index(val)\n",
    "            day_name.append(txt[num_position] + txt[num_position + 1] + txt[num_position +2])\n",
    "            break\n",
    "            \n",
    "    return day_name_dict[day_name[0]]\n",
    "# takes text and returns the first date in the document\n",
    "def extract_date(txt):\n",
    "    word_generator = (word for word in txt.split())\n",
    "    day_name = find_day(word_generator)\n",
    "    txt_start = int(txt.index(day_name))\n",
    "    txt_end = txt_start + 40\n",
    "    date_txt = txt[txt_start:txt_end].replace('\\n','')\n",
    "    cleaned_txt = re.findall('.* \\d{4}', date_txt)\n",
    "    date_list = cleaned_txt[0].split()\n",
    "    clean_date_list = map(lambda x:x.strip(\",\"), date_list)\n",
    "    clean_date_string = \", \".join(clean_date_list)\n",
    "    date_obj = datetime.strptime(clean_date_string, '%A, %B, %d, %Y')\n",
    "    return date_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary using DefaultDict where the date of publication is the key, and the text of the pdf is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_path = r'C:/Users/aregel/Documents/springboard/Twitter-Politics/data_wrangle/data/'\n",
    "end_path = '.pdf'\n",
    "data_dict = defaultdict(list)\n",
    "for i in range(0,528):\n",
    "    file_path = start_path + str(i) + end_path\n",
    "    txt = convert_pdf_to_txt(file_path)\n",
    "    date_obj = extract_date(txt)\n",
    "    data_dict[date_obj].append(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe from the dictionary, with the key values as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_reg_dataframe = pd.DataFrame.from_dict(data_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_reg_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataframe with the text of each pdf tokenized, and the index as the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_fed_reg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle the dataframe, so that you only need to process the text once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_reg_data = r'C:/Users/aregel/Documents/springboard/Twitter-Politics/data_wrangle/data/fed_reg_date_index.pickle'\n",
    "fed_reg_dataframe.to_pickle(fed_reg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_reg_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
